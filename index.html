<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
    google.load("jquery", "1.3.2");
</script>
<script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
<style type="text/css">
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 400;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 1200px;
    }
    
    a:link,
    a:visited {
        color: #1F407A;
        text-decoration: none;
    }
    
    a:hover {
        color: #1269B0;
    }
    
    h1,
    h2,
    h3,
    h4 {
        text-align: center;
    }
    
    h1 {
        font-weight: 450;
        line-height: 1.15em;
    }
    
    h2 {
        font-size: 1.75em;
        font-weight: 200;
        margin: 16px 0px 4px 0px;
    }
    
    h3 {
        font-weight: 300;
        font-size: 1.15em;
    }
    
    h4 {
        font-weight: 400;
        font-size: 1em;
    }
    
    .title {
        padding: 20px 0px 20px 0px;
    }
    
    section {
        margin: 16px 0px 16px 0px;
        text-align: justify;
        clear: both;
        line-height: 1.25em;
    }
    
    .author-row {
        font-size: 20px;
    }
    
    .affil-row {
        font-size: 22px;
    }
    
    .teaser {
        max-width: 100%;
    }
    
    .text-center {
        text-align: center;
    }
    
    .screenshot {
        width: 256px;
        border: 1px solid #ddd;
    }
    
    .screenshot-el {
        margin-bottom: 16px;
    }
    
    hr {
        height: 1px;
        border: 0;
        border-top: 1px solid #ddd;
        margin: 0;
    }
    
    .material-icons {
        vertical-align: -6px;
    }
    
    p {
        line-height: 1.25em;
    }
    
    .caption {
        font-size: 14px;
        /*font-style: italic;*/
        color: #666;
        text-align: left;
        margin-top: 6px;
        margin-bottom: 8px;
    }
    
    figure {
        display: block;
        margin: auto;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    
    #bibtex pre {
        font-size: 14px;
        background-color: #eee;
        padding: 16px;
    }
    
    .flex-row {
        display: flex;
        padding-top: 0px;
        flex-flow: row wrap;
        justify-content: space-around;
        line-height: 1.25em;
    }
    
    .paper-btn {
        position: relative;
        text-align: center;
        display: block;
        margin: 30px auto;
        padding: 8px 8px;
        border-width: 0;
        outline: none;
        border-radius: 2px;
        background-color: #2269a0;
        color: #d5e9ee !important;
        font-size: 20px;
        width: 100px;
        font-weight: 600;
    }
    
    .paper-btn:hover {
        opacity: 0.85;
    }
    
    .container {
        margin-left: auto;
        margin-right: auto;
        padding-left: 16px;
        padding-right: 16px;
        width: 1000px;
        text-align: center;
    }
    
    .col-5 {
        width: 20%;
        float: left;
    }
    
    .col-3 {
        width: 33%;
        float: left;
    }
    
    .col-2 {
        width: 50%;
        float: left;
    }
    
    .author-row p {
        text-align: center;
        line-height: 0px;
    }
    
    .author-row img {
        width: 57%;
        border-radius: 100%;
    }
    
    .author-row,
    .affil-row {
        overflow: auto;
        margin-top: 10px;
    }
    
    .glb-row {
        overflow: auto;
        margin-top: 20px;
        width: 1200px;
    }
    
    .centered {
        display: block;
        margin-left: auto;
        margin-right: auto;
    }
    
    .button_row {
        display: flex;
        width: 600px;
    }
    
    .bs {
        background-color: rgb(45, 77, 182);
        border: 1px solid rgb(195, 195, 195);
        color: white;
        width: 120px;
        height: 40px;
        font-size: 0.9em;
        font-weight: 500;
        margin: 15px;
        box-shadow: 2px 2px rgb(195, 195, 195), 2px 2px rgb(195, 195, 195), 1px 1px rgb(195, 195, 195);
    }
    
    .bs:hover {
        opacity: 0.85;
    }
</style>

<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>

<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>

<head>
    <title>PREDATOR: Registration of 3D Point Clouds with Low Overlap</title>
    <meta property="og:description" content="PREDATOR: Registration of 3D Point Clouds with Low Overlap" />
    <script src="https://kit.fontawesome.com/6e21e18363.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:creator" content="@ShengyHuang">
    <meta name="twitter:title" content="PREDATOR: Registration of 3D Point Clouds with Low Overlap">
    <meta name="twitter:description" content="">
    <meta name="twitter:image" content="https://github.com/overlappredator/overlappredator.github.io/assets/teaser.jpeg">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-6HHDEXF452"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'G-6HHDEXF452');
    </script>

</head>

<body>
    <div class="container">
        <div class="title">
            <h1>PREDATOR: Registration of 3D Point Clouds with Low Overlap</h1>
        </div>

        <div class="centered">
            <div class="author-row">
                <div class="col-5 text-center">
                    <a href="https://shengyuh.github.io/"><img src="assets/shengyu_huang.jpg">
                        <p>Shengyu Huang<sup>*,1</sup></p>
                    </a>
                </div>
                <div class="col-5 text-center">
                    <a href="https://zgojcic.github.io/"><img src="assets/zan_gojcic.png">
                        <p>Zan Gojcic<sup>*,1</sup></p>
                    </a>
                </div>
                <div class="col-5 text-center">
                    <a href="https://aelphy.github.io/"><img src="assets/mikhail_Usvyatsov.png">
                        <p>Mikhail Usvyatsov<sup>1</sup></p>
                    </a>
                </div>
                <div class="col-5 text-center">
                    <a href="https://baug.ethz.ch/en/department/people/staff/personen-detail.MTg3NzU5.TGlzdC82NzksLTU1NTc1NDEwMQ==.html"><img src="assets/andreas_wieser.png"><br>
                        <p>Andreas Wieser<sup>1</sup></p>
                    </a>
                </div>
                <div class="col-5 text-center">
                    <a href="https://igp.ethz.ch/personen/person-detail.html?persid=143986"><img src="assets/konrad_schindler.png"><br>
                        <p>Konrad Schindler<sup>1</sup></p>
                    </a>
                </div>
            </div>

            <div class="affil-row">
                <div class="col-2 text-center"><a href="https://igp.ethz.ch/"><sup>1</sup>ETH Zurich</a></div>
                <div class="col-2 text-center"><a href="https://igp.ethz.ch/"><sup>*</sup>Equal contribution</a></div>
            </div>

            <div class="conference-row">
                <div class="col-1 text-center">
                    <a href="http://cvpr2021.thecvf.com">CVPR 2021, Oral</a>
                </div>
            </div>

        </div>
        <p></p>

        <div class="parent">
            <a href="https://arxiv.org/pdf/2011.13005.pdf"><button class="bs"><span class="fa fa-file-pdf-o fa-fw"></span> Paper</button></a>
            <a href="assets/predator_poster.pdf"><button class="bs"><span class="fa fa-file-pdf-o fa-fw"></span> Poster</button></a>
            <a href="https://github.com/ShengyuH/OverlapPredator"><button class="bs"><span class="fa fa-github fa-fw"></span> Code</button></a>
            <a href="https://share.phys.ethz.ch/~gsg/Predator/"><button class="bs"><span class="fa fa-database fa-fw"></span> Data</button></a>
        </div>
        <p></p>
        <iframe width="840" height="472" src="https://www.youtube.com/embed/5L2vtuQL8Lg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        <section id="teaser">
            <a href="assets/network.png">
                <img class="centered" width="100%" src="assets/network.png">
            </a>
            <p class="caption">
                <div stype=font-size:10px>
                    Our method consumes two overlapping point clouds and estimates overlap heatmap, matchability heatmap, and point-wise features. The combined heatmap can be used to guide interest point sampling and to enable robust regsitration under low-overlap regime.
                </div>
            </p>
        </section>
        <br>
        <h2>Abstract</h2>
        <hr>
        <div class="flex-row">
            <div style="width: 54%; font-size:21px">
                <section id="abstract">
                    We introduce PREDATOR, a model for pairwise <b>p</b>oint-cloud <b>re</b>gistration with <b>d</b>eep <b>at</b>tention to the <b>o</b>verlap <b>r</b>egion. Different from previous work, our model is specifically designed to handle (also)
                    point-cloud pairs with low overlap. Its key novelty is an overlap-attention block for early information exchange between the latent encodings of the two point clouds. In this way the subsequent decoding of the latent representations
                    into per-point features is conditioned on the respective other point cloud, and thus can predict which points are not only salient, but also lie in the overlap region between the two point clouds. The ability to focus on points that
                    are relevant for matching greatly improves performance: PREDATOR raises the rate of successful registrations by more than 20% in the low-overlap scenario, and also sets a new state of the art for the <i>3DMatch</i> benchmark with 89%
                    registration recall.
                </section>
            </div>
            <div style="width: 46%">
                <section id="abstract">
                    <figure style="padding-left: 24px; padding-top: 8px; margin-bottom: 0">
                        <img width="100%" src="assets/teaser.jpeg">
                    </figure>
                </section>
            </div>
        </div>

        <br>
        <h2>Qualitative results</h2>
        <hr>
        <section id="qualitative results">
            <div style="font-size:21px">
                We evaluate PREDATOR and justify our design choices on real scan data, using <i>3DMatch</i> and <i>3DLoMatch</i>. Additionally, we compare PREDATOR to direct registration methods on the synthetic, object-centric <i>ModelNet40</i> and also
                showcase the effectiveness of PREDATOR on outdoor real scan <i>odometryKITTI</i>.
        </section>
        <figure style=m argin-bottom: 0 "></figure>
        <img class="centered " width="80% " src="assets/qualitative.jpeg ">
        <br>

        <div style="font-size:21px ">
        <h3>More qualitative results on <i>3DMatch</i></h3>
        <figure style= margin-bottom: 0"></figure>
        <img class="centered" width="100%" src="assets/3dmatch_supp.jpg">
        </a>
        </div>


        <div class="container">

            <h2>Citation</h2>
            <hr>
            <section id="bibtex">
                <pre><code>@inproceedings{predator,
                title={PREDATOR: Registration of 3D Point Clouds with Low Overlap},
                author={Shengyu Huang and Zan Gojcic and Mikhail Usvyatsov and Andreas Wieser, Konrad Schindler},
                booktitle={IEEE Conference on Computer Vision and Pattern Recognition, CVPR},
                year={2021}
                }
}</code></pre>
            </section>
        </div>
</body>

</html>